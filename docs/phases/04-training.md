# Training

Running training experiments and evaluating checkpoints.

## Goal

Fine-tune a base model on your synthesized training data. LoRA training teaches behavior and voice efficiently without modifying the full model.

## Training Configuration

Create a config file in `training/configs/`:

```yaml
# training/configs/my-experiment.yaml
base_model: Qwen/Qwen3-30B-A3B
dataset: default         # References training/data/default.yaml recipe
batch_size: 32
lora_rank: 32
max_length: 8192
epochs: 2
```

The `dataset` field references a recipe by name. When training starts, ISF
checks if the dataset is stale and warns you.

Common settings:

| Setting | Description | Typical Value |
|---------|-------------|---------------|
| `base_model` | HuggingFace model ID | `Qwen/Qwen3-30B-A3B` |
| `batch_size` | Samples per batch | 16-64 |
| `lora_rank` | LoRA adapter size | 16-64 |
| `max_length` | Max sequence length | 4096-8192 |
| `epochs` | Full passes over data | 1-3 |

## Choosing a Base Model

Training requires Tinker. Your `base_model` must be a model that Tinker can
train.

For early experiments, `Qwen/Qwen3-30B-A3B` is a strong default with a good
cost-to-capability tradeoff. The next tier of models is much larger (roughly an
order of magnitude more active parameters), such as Qwen3-235B, DeepSeek V3, or
Kimi K2.

## Preparing Training Data

Pipeline outputs live in `training/data/<pipeline-name>.jsonl`. Before training,
you combine these into a single dataset using a **recipe**.

### Dataset Recipes

Recipes are YAML files in `training/data/` that define how to combine pipeline outputs:

```yaml
# training/data/default.yaml
mode: simple  # Use all available data
categories:
  identity:
    pipelines:
      - identity-augmentation
      - wildchat-training
  knowledge:
    pipelines:
      - knowledge-qa
```

Categories are just organizational labels. The recipe combines all specified
pipelines into one training file.

For weighted sampling (balance categories):

```yaml
# training/data/balanced.yaml
mode: weighted
categories:
  identity:
    weight: 0.7  # 70% of samples
    pipelines:
      - identity-augmentation
  knowledge:
    weight: 0.3  # 30% of samples
    pipelines:
      - knowledge-qa
```

### Preparing a Dataset

```bash
# Check dataset status (is it stale?)
isf train data status

# Prepare dataset from recipe
isf train data prep default

# Preview without writing
isf train data prep default --dry-run

# Force rebuild even if current
isf train data prep balanced --force
```

The output is written to `training/data/prepared/<recipe-name>.jsonl` with a
manifest file for staleness tracking.

### Staleness Detection

Datasets track their source pipelines. If a pipeline is re-run, the dataset
becomes stale:

```bash
$ isf train data status
default: STALE
  - Source pipeline 'identity-augmentation' output is newer than dataset
  Run: isf train data prep default

balanced: CURRENT (1500 samples)
```

The training command also checks for staleness and warns you:

```bash
$ isf train run config.yaml --data training/data/prepared/default.jsonl
Error: Dataset 'default' has staleness issues:
  - Source pipeline 'identity-augmentation' is stale

To fix:
  isf train data prep default

Or use --allow-stale to proceed anyway.
```

## Running Training

```bash
# Run with config (uses dataset reference from config)
isf train run training/configs/my-experiment.yaml

# Or specify data file directly
isf train run training/configs/my-experiment.yaml --data training/data/prepared/default.jsonl

# Override settings
isf train run training/configs/my-experiment.yaml --epochs 3 --lr 0.0001

# Custom experiment name (optional)
isf train run training/configs/my-experiment.yaml --name my-experiment
```

**Experiment naming:** If you don't specify `--name`, experiments are auto-named `e001`, `e002`, etc. Custom names can be anything. Store multiple configs in `training/configs/` and use `isf train list` to see what is available.

Experiments are saved to `training/logs/<experiment-name>/`.

## Monitoring Progress

During training, `metrics.jsonl` logs progress:

```bash
# Watch training progress
tail -f training/logs/E001/metrics.jsonl | jq .
```

Key metrics to watch:
- `loss` - Should decrease over time
- `grad_norm` - Sudden spikes may indicate instability
- `eval_loss` - If using test split, should track train loss

## Managing Experiments

```bash
# List experiments
isf train list

# Show experiment details
isf train show E001
```

## Checkpoints

Training saves checkpoints at configured intervals. Final checkpoint is named `<experiment>-final`.

Intermediate checkpoints allow recovery if training fails partway through.

## Registering Trained Models

`registry.json` governs access to prompted and trained models. It is generated by
ISF and should not be edited directly.

After training completes, the checkpoint is automatically registered. You can also
manually rebuild the registry:

```bash
isf registry build
```

This rebuilds `registry.json` and registers each experiment's final checkpoint
as a model name (e.g., `e007`). Use `isf registry list` to see what's available.

Intermediate checkpoints are not auto-registered. If you need them, reference
the full checkpoint path directly or add an explicit entry to `isf.yaml` and
rebuild the registry.

## Evaluating Trained Models

After training, run the same evals against the trained checkpoint:

```bash
# Eval trained model
isf eval run my-identity E001-final --limit 50

# Compare to prompted baseline
isf eval run my-identity yourmodel-dev-full --limit 50
```

Key questions:
- Did scores improve over the prompted baseline?
- Is the identity more consistent?
- Any capability degradation?

## Common Issues

### Training diverges
**Symptom**: Loss increases or oscillates wildly
**Fix**: Lower learning rate, reduce batch size, check data quality

### Overfitting
**Symptom**: Train loss good, eval loss poor
**Fix**: Reduce epochs, add more diverse data, use early stopping

### Capability degradation
**Symptom**: Model loses general abilities
**Fix**: Include general capability data in training, not just identity

### Identity not learned
**Symptom**: Trained model still sounds generic
**Fix**: Check that identity doc is distinctive, review training data quality, possibly train longer

## Iterating

Training rarely works perfectly first try. Common adjustments:

- **Add more data**: If identity isn't sticking, more examples help
- **Adjust epochs**: Too few = underfit, too many = overfit
- **Revisit identity doc**: If trained model is wrong in consistent ways, the identity specification may need revision
- **Fix data quality**: Bad training data produces bad models

## Validation Checklist

Before shipping a trained model:

- [ ] **Training completed**: No crashes or early termination
- [ ] **Loss looks healthy**: Decreased over time, no wild swings
- [ ] **Evals improved**: Better scores than prompted baseline
- [ ] **Capabilities intact**: General abilities not degraded
- [ ] **Spot-checked**: Manually tested several conversations

If any fail, iterate. The goal is a model that reliably exhibits the identity while remaining generally capable.
